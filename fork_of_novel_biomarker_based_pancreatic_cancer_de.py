{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1731978,"sourceType":"datasetVersion","datasetId":1027924}],"dockerImageVersionId":30513,"isInternetEnabled":false,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n# This Python 3 environment,running on a jupyter notebook\n# Useful packages used for visualization and effeciency\n\nimport numpy as np # linear algebra computing\nimport pandas as pd # for data processing\nimport matplotlib.pyplot as plt # for visualization of data\nimport seaborn as sns # for visualization of data\n\n# %% [markdown]\n# According to the information given previosuly, I decided to keep only the following columns as important variables for prediction:\n\n# %% [code]\ndf = pd.read_csv('/kaggle/input/urinary-biomarkers-for-pancreatic-cancer/Debernardi et al 2020 data.csv', usecols=['creatinine', 'LYVE1','age', 'sex','TFF1','plasma_CA19_9', 'REG1A', 'REG1B', 'diagnosis'])\ndf['diagnosis'] = df['diagnosis'] == 3\ndf['sex'] = df['sex'].map({'M': 1, 'F': 0})\n\n# sample data\ndf.head(20)\n\n# %% [code]\n# true and false pair plot, and their correlation with the hyperparamters\nsns.pairplot(data = df, vars=['REG1A','creatinine','TFF1', 'LYVE1','plasma_CA19_9', 'REG1B','age'], hue='diagnosis', diag_kind='kde', palette = 'Set1')\n\n# %% [markdown]\n# It seems that there is a higher frequency of low values for REG1B in negative diagnoses (FALSE) than in positive diagnoses (TRUE). Similarly, in the case of TFF1, the values appear to be higher in positive diagnoses, and small values seem to be more frequent in negative diagnoses. The distribution of LYVE1 values with respect to the diagnosis looks promising, as there is a higher probability of high values in the case of a positive diagnosis (TRUE), and a higher probability of low values in the case of negative diagnoses (FALSE) and the distributions are more separated.\n\n# %% [code]\n# confusion matrix with heat map, showing their correlation with the hyperparamters\ncorr = df.dropna().corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm')\n\n# %% [markdown] {\"execution\":{\"iopub.status.busy\":\"2023-06-21T20:16:20.773232Z\",\"iopub.execute_input\":\"2023-06-21T20:16:20.77358Z\",\"iopub.status.idle\":\"2023-06-21T20:16:20.78355Z\",\"shell.execute_reply.started\":\"2023-06-21T20:16:20.773552Z\",\"shell.execute_reply\":\"2023-06-21T20:16:20.781084Z\"}}\n# As we can observe, LYVE1 has a higher correlation with the diagnosis, just as we expected based on the visualization of the previous data.\n\n# %% [code]\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import plot_tree\nfrom sklearn.impute import SimpleImputer\n\nX = df.drop('diagnosis', axis=1)\ny = df['diagnosis']\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\nX_train_np = X_train.values\ny_train_np = y_train.values\n\nfrom sklearn.model_selection import cross_val_score\n\n# Define the pipeline with an extended grid of parameters\npipeline_rf = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('rf', RandomForestClassifier())\n])\n\n# Define an extended grid of parameters to search over\nparam_grid_rf = {\n    'rf__n_estimators': [100, 200, 300],\n    'rf__max_depth': [None, 5, 10, 15],\n    'rf__min_samples_split': [2, 5, 10],\n    'rf__min_samples_leaf': [1, 2, 4]\n}\n\n# Create the grid search object with cross-validation\ngrid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv=5, scoring='accuracy')\n\n# Fit the grid search object to the data\ngrid_search_rf.fit(X_train, y_train)\n\n# Print the best parameters and the best score\nprint(\"Best parameters (Random Forest): \", grid_search_rf.best_params_)\nprint(\"Best score (Random Forest): \", grid_search_rf.best_score_)\n\n# Evaluate training accuracy for Random Forest with cross-validation\ncv_scores_rf = cross_val_score(grid_search_rf.best_estimator_, X_train, y_train, cv=5)\nprint(\"Cross-Validation Scores (Random Forest):\", cv_scores_rf)\nprint(\"Mean CV Score (Random Forest):\", np.mean(cv_scores_rf))\n\n# Select the best model\nbest_model_rf = grid_search_rf.best_estimator_\n\n# Make predictions with the best model\ny_pred_rf = best_model_rf.predict(X_test)\n\n# Print the accuracy of the best model on the test set\nprint(\"Test set accuracy (Random Forest): \", accuracy_score(y_test, y_pred_rf))\n\n# Get feature importances from the best model\nimportances_rf = best_model_rf.named_steps['rf'].feature_importances_\n\n# Sort the feature importances in descending order and get the indices\nindices_rf = np.argsort(importances_rf)[::-1]\n\n# Plot the feature importances\nplt.figure()\nplt.title(\"Feature importances (Random Forest)\")\nplt.barh(range(X_train.shape[1]), importances_rf[indices_rf],\n       color=\"g\", align=\"center\")\nplt.yticks(range(X_train.shape[1]), X_train.columns[indices_rf])\nplt.ylim([-1, X_train.shape[1]])\nplt.show()\n\n# %% [markdown]\n# XGBoost and LighGBM can operate taking NaNs into account, so let's see how it performs below.\n\n# %% [code]\n# imports necessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import  classification_report, balanced_accuracy_score\n\n# LightGBM Classification\nimport lightgbm as lgb\n\nX = df.drop('diagnosis', axis=1)\ny = df['diagnosis']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Feature Scaling for LightGBM\npipeline_lgbm = Pipeline([\n    ('scaler', StandardScaler()),\n    ('lgbm', lgb.LGBMClassifier(objective='binary', is_unbalance=True, random_state=42))\n])\n\n# Define the extended grid of parameters to search over for LightGBM\nparam_grid_lgbm = {\n    'lgbm__n_estimators': [50, 100, 200],\n    'lgbm__learning_rate': [0.01, 0.1, 0.2],\n    'lgbm__max_depth': [5, 10, 15],\n    'lgbm__num_leaves': [31, 50, 100]\n}\n\n# Create the extended grid search object for LightGBM with cross-validation\ngrid_search_lgbm = GridSearchCV(pipeline_lgbm, param_grid_lgbm, cv=5, scoring='accuracy')\n\n# Fit the grid search object to the data for LightGBM\ngrid_search_lgbm.fit(X_train, y_train)\n\n# Print the best parameters and the best score for LightGBM\nprint(\"Best parameters (LightGBM): \", grid_search_lgbm.best_params_)\nprint(\"Best score (LightGBM): \", grid_search_lgbm.best_score_)\n\n# Evaluate training accuracy for LightGBM with cross-validation\ncv_scores_lgbm = cross_val_score(grid_search_lgbm.best_estimator_, X_train, y_train, cv=5)\nprint(\"Cross-Validation Scores (LightGBM):\", cv_scores_lgbm)\nprint(\"Mean CV Score (LightGBM):\", np.mean(cv_scores_lgbm))\n\n# Select the best model for LightGBM\nbest_model_lgbm = grid_search_lgbm.best_estimator_\n\n# Make predictions with the best LightGBM model\ny_pred_lgbm = best_model_lgbm.predict(X_test)\n\n# Print the classification report for LightGBM\nprint(\"Classification report (LightGBM): \\n\", classification_report(y_test, y_pred_lgbm))\n\n# Print the balanced accuracy for LightGBM\nprint(\"Balanced accuracy (LightGBM): \", balanced_accuracy_score(y_test, y_pred_lgbm))\n\n# Get feature importances from the best LightGBM model\nimportances_lgbm = best_model_lgbm.named_steps['lgbm'].feature_importances_\n\n# Sort the feature importances in descending order and get the indices\nindices_lgbm = np.argsort(importances_lgbm)[::-1]\n\n# Plot the feature importances for LightGBM\nplt.figure()\nplt.title(\"Feature importances (LightGBM)\")\nplt.barh(range(X_train.shape[1]), importances_lgbm[indices_lgbm],\n       color=\"r\", align=\"center\")\nplt.yticks(range(X_train.shape[1]), X_train.columns[indices_lgbm])\nplt.ylim([-1, X_train.shape[1]])\nplt.show()\n\n# %% [code]\n# XGBoost Classification\n# preprocesses the data, kind of makes the first iteration of the model\npipeline_xgb = Pipeline([\n    ('scaler', StandardScaler()),\n    ('xgb', XGBClassifier(eval_metric='logloss', objective='binary:logistic', scale_pos_weight=sum(y_train == 0) / sum(y_train == 1)))\n])\n\n# how it will find its paramaters\nparam_grid_xgb = {\n    'xgb__n_estimators': [200, 300, 400],\n    'xgb__max_depth': [3, 5, 7],\n    'xgb__learning_rate': [0.05, 0.1, 0.2],\n    'xgb__reg_alpha': [0, 0.1, 0.5], #tests a bunch of paramaters to see what works best\n    'xgb__reg_lambda': [0, 0.1, 0.5], #ensures model is not overfit to this specfic data, so can work well with new inputs of data\n    'xgb__subsample': [0.8, 0.9, 1.0], #controls how much data xgboost sees\n    'xgb__colsample_bytree': [0.8, 0.9, 1.0] #subsamples data again for every tree\n}\n\n# cross-validation with grid search\ngrid_search_xgb = GridSearchCV(pipeline_xgb, param_grid_xgb, cv=5, scoring='accuracy', n_jobs=-1)\n\n# fits the data from grid search to the data for XGBoost\ngrid_search_xgb.fit(X_train, y_train)\n\n# the best parameters and the best score for XGBoost\nprint(\"Best parameters (XGBoost): \", grid_search_xgb.best_params_)\nprint(\"Best score (XGBoost): \", grid_search_xgb.best_score_)\n\n# calculate training accuracy for XGBoost after cross-validation\ncv_scores_xgb = cross_val_score(grid_search_xgb.best_estimator_, X_train, y_train, cv=5)\nprint(\"Cross-Validation Scores (XGBoost):\", cv_scores_xgb)\nprint(\"Mean CV Score (XGBoost):\", np.mean(cv_scores_xgb))\n\n#  the best model for XGBoost, by using grid search(tries diff values and spits out best one)\nbest_model_xgb = grid_search_xgb.best_estimator_\n\n# Make predictions with the best XGBoost model\ny_pred_xgb = best_model_xgb.predict(X_test)\n\n# the classification report for XGBoost\nprint(\"Classification report (XGBoost): \\n\", classification_report(y_test, y_pred_xgb))\n\n# the balanced accuracy for XGBoost\nprint(\"Balanced accuracy (XGBoost): \", balanced_accuracy_score(y_test, y_pred_xgb))\n\n# feature importances from the best XGBoost model, basically just the hyperparamters\nimportances_xgb = best_model_xgb.named_steps['xgb'].feature_importances_\n\n# the feature importances in descending order, so it looks nice on the graph\nindices_xgb = np.argsort(importances_xgb)[::-1]\n\n# the graph of the feature importances for XGBoost\nplt.figure()\nplt.title(\"Feature importances (XGBoost)\")\nplt.barh(range(X_train.shape[1]), importances_xgb[indices_xgb],\n       color=\"b\", align=\"center\")\nplt.yticks(range(X_train.shape[1]), X_train.columns[indices_xgb])\nplt.ylim([-1, X_train.shape[1]])\nplt.show()\n\n# %% [code]\nfrom sklearn.ensemble import VotingClassifier\n\n# Define the individual classifiers\nrf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\nlgbm_classifier = lgb.LGBMClassifier(objective='binary', is_unbalance=True, random_state=42)\nxgb_classifier = XGBClassifier(eval_metric='logloss', objective='binary:logistic', scale_pos_weight=sum(y_train == 0) / sum(y_train == 1), random_state=42)\n\n# Create a voting classifier with custom weights\nweights = [2, 1]\nweights_normalized = [weight / sum(weights) for weight in weights]\n\nvoting_classifier = VotingClassifier(\n    estimators=[\n        ('lgbm', lgbm_classifier),\n        ('xgb', xgb_classifier)\n    ],\n    voting='soft',\n    weights=weights_normalized\n)\n\n# Create a pipeline for the voting classifier\npipeline_voting = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('voting', voting_classifier)\n])\n\n# Fit the pipeline to the data\npipeline_voting.fit(X_train, y_train)\n\n# Make predictions with the voting classifier\ny_pred_voting = pipeline_voting.predict(X_test)\n\n# Print the accuracy of the voting classifier on the test set\nprint(\"Test set accuracy (Voting): \", accuracy_score(y_test, y_pred_voting))","metadata":{"_uuid":"a20604d8-3619-4f6f-8409-3db4fb587bcf","_cell_guid":"b68bdb39-56de-4724-8be9-4805f964dde0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}